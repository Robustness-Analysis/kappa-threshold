noise_level = numeric(),
threshold = numeric(),
critical_percentage = numeric(),
critical_kappa = numeric(),
stringsAsFactors = FALSE
)
# Create grid for parameters
threshold_grid <- expand.grid(
dataset = dataset_names,
tech = techniques,
noise_lvl = noise_levels,
thresh_lvl = kl_thresholds,
stringsAsFactors = FALSE
)
# Apply function (tidyverse)
results_list <- purrr::pmap(threshold_grid, function(dataset, tech, noise_lvl, thresh_lvl) {
# Filter data
current_data <- klc_data %>%
filter(dataset_name == dataset, technique == tech, noise == noise_lvl)
cat("*------------------------------------*\n")
cat("Processing:", dataset, "-", tech, "| Noise Level:", noise_lvl, "| Threshold:", thresh_lvl, "\n")
if (nrow(current_data) > 0) {
# Apply find_threshold function
result <- find_threshold(current_data, thresh_lvl)
# Return results as a data frame row
return(data.frame(
dataset_name = dataset,
technique = tech,
noise_level = noise_lvl,
threshold = thresh_lvl,
critical_percentage = result$critical_percentage,
critical_kappa = result$kappa_loss,
stringsAsFactors = FALSE
))
} else {
# Return NA entries if no data is available
return(data.frame(
dataset_name = dataset,
technique = tech,
noise_level = noise_lvl,
threshold = thresh_lvl,
critical_percentage = NA,
critical_kappa = NA,
stringsAsFactors = FALSE
))
}
})
cat("*------------------------------------*\n")
# Combine all results into one data frame
threshold_results <- do.call(rbind, results_list)
# Save the results
write.csv(threshold_results, file = "results/threshold_results.csv", row.names = FALSE)
cat("Results recorded in: 'results/threshold_results.csv'\n")
# Display the raw data from CSV
kable(head(results, 20), caption = "Raw Data from threshold_results.csv (First 20 Rows)")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
fig.width = 10, fig.height = 7)
# Load required packages
library(tidyverse)
library(ggplot2)
library(plotly)
library(DT)
library(viridis)
library(patchwork)
library(knitr)
# Load data
results <- read.csv("results/threshold_results.csv", stringsAsFactors = FALSE)
# Convert critical_percentage to numeric
results$critical_percentage <- as.numeric(as.character(results$critical_percentage))
results$critical_kappa <- as.numeric(as.character(results$critical_kappa))
# Display the raw data from CSV
kable(head(results, 20), caption = "Raw Data from threshold_results.csv (First 20 Rows)")
# Create interactive table of the full dataset
datatable(results,
options = list(
pageLength = 10,
scrollX = TRUE,
dom = 'Bfrtip',
buttons = c('copy', 'csv', 'excel')
),
extensions = 'Buttons',
caption = "Complete Raw Data from threshold_results.csv")
# Summary statistics by threshold level
threshold_summary <- results %>%
group_by(threshold) %>%
summarise(
mean_critical = mean(critical_percentage, na.rm = TRUE),
median_critical = median(critical_percentage, na.rm = TRUE),
)
kable(threshold_summary, caption = "Summary of Critical Percentages by Threshold Level")
# Summary by technique
technique_summary <- results %>%
group_by(technique) %>%
summarise(
mean_critical = mean(critical_percentage, na.rm = TRUE),
median_critical = median(critical_percentage, na.rm = TRUE),
) %>%
arrange(median_critical)
kable(technique_summary, caption = "Summary of Critical Percentages by ML Technique")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
fig.width = 10, fig.height = 7)
# Load required packages
library(tidyverse)
library(ggplot2)
library(plotly)
library(DT)
library(viridis)
library(patchwork)
library(knitr)
# Load data
results <- read.csv("results/threshold_results.csv", stringsAsFactors = FALSE)
klc_data <- readRDS("results/KLC_plot_deciles.rds")
# Clean up data
results$critical_percentage <- as.numeric(as.character(results$critical_percentage))
results$critical_kappa <- as.numeric(as.character(results$critical_kappa))
klc_data <- klc_data %>%
filter(noise %in% c(10, 20, 30)) %>%
select(-c(accuracy, kappa, dataset_order, method_order))
# Display the raw data from CSV
kable(head(results, 20), caption = "Raw Data from threshold_results.csv (First 20 Rows)")
# Create interactive table of the full dataset
datatable(results,
options = list(
pageLength = 10,
scrollX = TRUE,
dom = 'Bfrtip',
buttons = c('copy', 'csv', 'excel')
),
extensions = 'Buttons',
caption = "Complete Raw Data from threshold_results.csv")
# Summary statistics by threshold level
threshold_summary <- results %>%
group_by(threshold) %>%
summarise(
mean_critical = mean(critical_percentage, na.rm = TRUE),
median_critical = median(critical_percentage, na.rm = TRUE),
)
kable(threshold_summary, caption = "Summary of Critical Percentages by Threshold Level")
# Summary by technique
technique_summary <- results %>%
group_by(technique) %>%
summarise(
mean_critical = mean(critical_percentage, na.rm = TRUE),
median_critical = median(critical_percentage, na.rm = TRUE),
) %>%
arrange(median_critical)
kable(technique_summary, caption = "Summary of Critical Percentages by ML Technique")
# Get unique dataset names and technique names
datasets <- unique(klc_data$dataset_name)
method_names <- unique(klc_data$technique)
# Create a new column to control the order of datasets
klc_data$dataset_order <- factor(klc_data$dataset_name, levels = datasets)
# Create a new column to control the order of methods
klc_data$method_order <- factor(klc_data$technique, levels = method_names)
# Create custom labels for methods (a-u) and datasets (1-26)
method_labels <- letters[1:length(method_names)]
names(method_labels) <- method_names
dataset_labels <- as.character(1:length(datasets))
names(dataset_labels) <- datasets
# Create plot
p <- ggplot(klc_data, aes(x = percentage, y = kappa_loss, color = factor(noise))) +
geom_point(size = 0.8, alpha = 0.6) +
geom_line(aes(group = factor(noise)), linewidth = 0.8) +
labs(title = "Kappa Loss Curves by Dataset, Technique, and Noise Level",
x = "Percentage of Instances",
y = "Kappa Loss",
color = "Noise Level") +
theme_bw() +
scale_y_continuous(limits = c(0.0, 1), breaks = seq(0, 1, by = 0.2)) +
facet_grid(method_order ~ dataset_order, scales = "free",
labeller = labeller(method_order = method_labels, dataset_order = dataset_labels)) +
theme(strip.text = element_text(size = 7),
axis.text = element_text(size = 6),
legend.position = "bottom")
# Print plot
print(p)
ggplot(results %>% filter(!is.na(critical_percentage)),
aes(x = critical_percentage, fill = as.factor(threshold))) +
geom_density(alpha = 0.7) +
labs(title = "Distribution of Critical Percentages by Threshold Level",
x = "Critical Percentage of Instances",
y = "Density",
fill = "Threshold") +
theme_minimal() +
scale_fill_manual(values = c("0.05" = "#94D2BD", "0.1" = "#0A9396", "0.15" = "#005F73")) +
theme(legend.position = "bottom")
# Calculate average kappa loss by threshold
kappa_loss_by_threshold <- results %>%
filter(!is.na(critical_kappa)) %>%
group_by(threshold) %>%
summarise(
mean_kappa_loss = mean(critical_kappa, na.rm = TRUE),
median_kappa_loss = median(critical_kappa, na.rm = TRUE),
)
# Display summary table
kable(kappa_loss_by_threshold,
caption = "Summary of Kappa Loss by Threshold Level",
digits = 3)
# Calculate average kappa loss by technique and threshold
kappa_loss_by_technique <- results %>%
filter(!is.na(critical_kappa)) %>%
group_by(technique, threshold) %>%
summarise(
mean_kappa_loss = mean(critical_kappa, na.rm = TRUE),
n_values = sum(!is.na(critical_kappa))
) %>%
ungroup()
# Create a heatmap of kappa loss by technique and threshold
ggplot(kappa_loss_by_technique, aes(x = as.factor(threshold), y = reorder(technique, -mean_kappa_loss), fill = mean_kappa_loss)) +
geom_tile() +
scale_fill_viridis_c() +
labs(title = "Average Kappa Loss by ML Technique and Threshold",
x = "Threshold",
y = "Machine Learning Technique",
fill = "Mean Kappa Loss") +
theme_minimal() +
theme(axis.text.y = element_text(size = 8)) +
geom_text(aes(label = sprintf("%.2f", mean_kappa_loss)), size = 3)
# Create dataset information summary
dataset_info <- results %>%
filter(!is.na(critical_percentage)) %>%
group_by(dataset_name, technique, noise_level) %>%
summarise(
max_instances = max(critical_percentage, na.rm = TRUE),
mean_kappa_loss = mean(critical_kappa, na.rm = TRUE),
n_thresholds = n()
) %>%
ungroup()
# Save to a dataframe
dataset_info_df <- as.data.frame(dataset_info)
# Display first few rows of the table
kable(head(dataset_info_df, 20),
caption = "Dataset, Technique, Noise Level and Maximum Instance Information",
digits = 3)
# Save the complete dataset info to an RDS file
saveRDS(dataset_info_df, "results/dataset_technique_info.rds")
# Create interactive table
datatable(dataset_info_df,
options = list(
pageLength = 10,
scrollX = TRUE,
dom = 'Bfrtip',
buttons = c('copy', 'csv', 'excel')
),
extensions = 'Buttons',
caption = "Complete Dataset, Technique, and Instance Information")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
fig.width = 10, fig.height = 7)
# Load required packages
library(tidyverse)
library(ggplot2)
library(plotly)
library(DT)
library(viridis)
library(patchwork)
library(knitr)
# Load data
results <- read.csv("results/threshold_results.csv", stringsAsFactors = FALSE)
klc_data <- readRDS("results/KLC_plot_deciles.rds")
# Clean up data
results$critical_percentage <- as.numeric(as.character(results$critical_percentage))
results$critical_kappa <- as.numeric(as.character(results$critical_kappa))
klc_data <- klc_data %>%
filter(noise %in% c(10, 20, 30)) %>%
select(-c(accuracy, kappa, dataset_order, method_order))
# Display the raw data from CSV
kable(head(results, 20), caption = "Raw Data from threshold_results.csv (First 20 Rows)")
# Create interactive table of the full dataset
datatable(results,
options = list(
pageLength = 10,
scrollX = TRUE,
dom = 'Bfrtip',
buttons = c('copy', 'csv', 'excel')
),
extensions = 'Buttons',
caption = "Complete Raw Data from threshold_results.csv")
# Summary statistics by threshold level
threshold_summary <- results %>%
group_by(threshold) %>%
summarise(
mean_critical = mean(critical_percentage, na.rm = TRUE),
median_critical = median(critical_percentage, na.rm = TRUE),
)
kable(threshold_summary, caption = "Summary of Critical Percentages by Threshold Level")
# Summary by technique
technique_summary <- results %>%
group_by(technique) %>%
summarise(
mean_critical = mean(critical_percentage, na.rm = TRUE),
median_critical = median(critical_percentage, na.rm = TRUE),
) %>%
arrange(median_critical)
kable(technique_summary, caption = "Summary of Critical Percentages by ML Technique")
# Get unique dataset names and technique names
datasets <- unique(klc_data$dataset_name)
method_names <- unique(klc_data$technique)
# Create a new column to control the order of datasets
klc_data$dataset_order <- factor(klc_data$dataset_name, levels = datasets)
# Create a new column to control the order of methods
klc_data$method_order <- factor(klc_data$technique, levels = method_names)
# Create custom labels for methods (a-u) and datasets (1-26)
method_labels <- letters[1:length(method_names)]
names(method_labels) <- method_names
dataset_labels <- as.character(1:length(datasets))
names(dataset_labels) <- datasets
# Create plot
p <- ggplot(klc_data, aes(x = percentage, y = kappa_loss, color = factor(noise))) +
geom_point(size = 0.8, alpha = 0.6) +
geom_line(aes(group = factor(noise)), linewidth = 0.8) +
labs(title = "Kappa Loss Curves by Dataset, Technique, and Noise Level",
x = "Percentage of Instances",
y = "Kappa Loss",
color = "Noise Level") +
theme_bw() +
scale_y_continuous(limits = c(0.0, 1), breaks = seq(0, 1, by = 0.2)) +
facet_grid(method_order ~ dataset_order, scales = "free",
labeller = labeller(method_order = method_labels, dataset_order = dataset_labels)) +
theme(strip.text = element_text(size = 7),
axis.text = element_text(size = 6),
legend.position = "bottom")
# Print plot
print(p)
ggplot(results %>% filter(!is.na(critical_percentage)),
aes(x = critical_percentage, fill = as.factor(threshold))) +
geom_density(alpha = 0.7) +
labs(title = "Distribution of Critical Percentages by Threshold Level",
x = "Critical Percentage of Instances",
y = "Density",
fill = "Threshold") +
theme_minimal() +
scale_fill_manual(values = c("0.05" = "#94D2BD", "0.1" = "#0A9396", "0.15" = "#005F73")) +
theme(legend.position = "bottom")
# Calculate average kappa loss by threshold
kappa_loss_by_threshold <- results %>%
filter(!is.na(critical_kappa)) %>%
group_by(threshold) %>%
summarise(
mean_kappa_loss = mean(critical_kappa, na.rm = TRUE),
median_kappa_loss = median(critical_kappa, na.rm = TRUE),
)
# Display summary table
kable(kappa_loss_by_threshold,
caption = "Summary of Kappa Loss by Threshold Level",
digits = 3)
# Calculate average kappa loss by technique and threshold
kappa_loss_by_technique <- results %>%
filter(!is.na(critical_kappa)) %>%
group_by(technique, threshold) %>%
summarise(
mean_kappa_loss = mean(critical_kappa, na.rm = TRUE),
n_values = sum(!is.na(critical_kappa))
) %>%
ungroup()
# Create a heatmap of kappa loss by technique and threshold
ggplot(kappa_loss_by_technique, aes(x = as.factor(threshold), y = reorder(technique, -mean_kappa_loss), fill = mean_kappa_loss)) +
geom_tile() +
scale_fill_viridis_c() +
labs(title = "Average Kappa Loss by ML Technique and Threshold",
x = "Threshold",
y = "Machine Learning Technique",
fill = "Mean Kappa Loss") +
theme_minimal() +
theme(axis.text.y = element_text(size = 8)) +
geom_text(aes(label = sprintf("%.2f", mean_kappa_loss)), size = 3)
# Create dataset information summary
dataset_info <- results %>%
filter(!is.na(critical_percentage)) %>%
group_by(dataset_name, technique, noise_level) %>%
summarise(
max_instances = max(critical_percentage, na.rm = TRUE),
mean_kappa_loss = mean(critical_kappa, na.rm = TRUE),
n_thresholds = n()
) %>%
ungroup()
# Save to a dataframe
dataset_info_df <- as.data.frame(dataset_info)
# Display first few rows of the table
kable(head(dataset_info_df, 20),
caption = "Dataset, Technique, Noise Level and Maximum Instance Information",
digits = 3)
# Save the complete dataset info to an RDS file
saveRDS(dataset_info_df, "results/dataset_technique_info.rds")
# Create interactive table
datatable(dataset_info_df,
options = list(
pageLength = 10,
scrollX = TRUE,
dom = 'Bfrtip',
buttons = c('copy', 'csv', 'excel')
),
extensions = 'Buttons',
caption = "Complete Dataset, Technique, and Instance Information")
View(kappa_loss_by_threshold)
# Find best performing techniques at each threshold level
best_techniques <- results %>%
filter(!is.na(critical_percentage)) %>%
group_by(threshold, technique) %>%
summarise(mean_critical = mean(critical_percentage, na.rm = TRUE)) %>%
arrange(threshold, mean_critical) %>%
group_by(threshold) %>%
slice_head(n = 5)
kable(best_techniques, caption = "Top 5 Techniques with Lowest Mean Critical Percentages by Threshold")
View(best_techniques)
#!/usr/bin/env Rscript
# Load required libraries
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
tidyverse,  # for data manipulation and visualization
dplyr,       # for data manipulation
tidyr,       # for data reshaping
ggplot2      # for visualization
)
# Load dataset names, techniques, and instances levels
dataset_names <- readRDS("files/dataset_name.rds")
techniques <- readRDS("files/technique_name.rds")
instance_levels <- readRDS("files/instance_level.rds")
# Define noise levels
noise_levels <- c(10, 20, 30)
# Define thresholds for Kappa Loss
kl_thresholds <- c(0.05, 0.10, 0.15)
# Load the KLC curves
klc_data <- readRDS("results/KLC_plot_deciles.rds")
klc_data <- klc_data %>%
filter(noise %in% c(10, 20, 30)) %>%
select(-c(accuracy, kappa, dataset_order, method_order))
# Function to find the instance percentage where the threshold is exceeded
find_threshold <- function(data, threshold) {
# Order data by percentage of instances
data <- data[order(data$percentage), ]
# Find the first instance where kappa_loss exceeds the threshold
for (i in seq_len(nrow(data))) {
if (data$kappa_loss[i] > threshold) {
cat("Kappa loss:", data$kappa_loss[i], ", Critical instance:", data$percentage[i], "\n")
return(list(
critical_percentage = data$percentage[i],
kappa_loss = data$kappa_loss[i]
))
}
}
# If no point exceeds threshold, return 100%
if(nrow(data) > 0) {
cat("Kappa loss: 0, Critical instance: 100%", "\n")
return(list(
critical_percentage = 100,  # Return 100% as last point
kappa_loss = data$kappa_loss[nrow(data)]
))
} else { # If not found, return NA
cat("NA", "\n")
return(list(
critical_percentage = NA,
kappa_loss = NA
))
}
}
# Store threshold results
threshold_results <- data.frame(
dataset_name = character(),
technique = character(),
noise_level = numeric(),
threshold = numeric(),
critical_percentage = numeric(),
kappa_at_critical = numeric(),
stringsAsFactors = FALSE
)
# Create grid for parameters
threshold_grid <- expand.grid(
dataset = dataset_names,
tech = techniques,
noise_lvl = noise_levels,
thresh_lvl = kl_thresholds,
stringsAsFactors = FALSE
)
# Apply function (tidyverse)
results_list <- purrr::pmap(threshold_grid, function(dataset, tech, noise_lvl, thresh_lvl) {
# Filter data
current_data <- klc_data %>%
filter(dataset_name == dataset, technique == tech, noise == noise_lvl)
cat("*------------------------------------*\n")
cat("Processing:", dataset, "-", tech, "| Noise Level:", noise_lvl, "| Threshold:", thresh_lvl, "\n")
if (nrow(current_data) > 0) {
# Apply find_threshold function
result <- find_threshold(current_data, thresh_lvl)
# Return results as a data frame row
return(data.frame(
dataset_name = dataset,
technique = tech,
noise_level = noise_lvl,
threshold = thresh_lvl,
critical_percentage = result$critical_percentage,
kappa_at_critical = result$kappa_loss,
stringsAsFactors = FALSE
))
} else {
# Return NA entries if no data is available
return(data.frame(
dataset_name = dataset,
technique = tech,
noise_level = noise_lvl,
threshold = thresh_lvl,
critical_percentage = NA,
kappa_at_critical = NA,
stringsAsFactors = FALSE
))
}
})
cat("*------------------------------------*\n")
# Combine all results into one data frame
threshold_results <- do.call(rbind, results_list)
# Save the results
write.csv(threshold_results, file = "results/threshold_results.csv", row.names = FALSE)
cat("Results recorded in: 'results/threshold_results.csv'\n")
